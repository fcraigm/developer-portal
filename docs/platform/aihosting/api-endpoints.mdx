
## API Endpoints

The API can be accessed via HTTPS and is compatible with the OpenAI API. The base URL is `https://llm.aihosting.mittwald.de`. Every interaction with the API requires an `Authorization` header containing a valid API key. This key can be generated in mStudio.

We use `curl` in our examples as it's the simplest and fastest way to test. For production use, we recommend using frameworks and libraries that support OpenAI.

### `/v1/models`

This endpoint returns a list of available models.

```sh
export APIKEY=sk-…

curl -i -X GET https://llm.aihosting.mittwald.de/v1/models \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY"
```

A dictionary is returned containing a list of available models. The included `id` can be used as the model identifier for subsequent API routes.

### `/v1/chat/completions` and `/v1/completions`

This route allows content to be sent to the LLM in chat format. The endpoint supports streaming for the generated content.

```sh
export APIKEY=sk-…

curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Mistral-Small-3.2-24B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Moin und hallo!"
            }
        ]
    }'
```

The `model` parameter requires a valid model name, which can be retrieved via the `/v1/models` route. Additional model parameters like `temperature`, `top_p`, or `top_k` can be submitted. Recommended settings for these parameters are provided in the model descriptions if they differ from the default. The influence of advanced parameters on the response depends on the specific model.

To receive a stream response, the `stream: true` option must be set.

```sh
curl -i -N -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Mistral-Small-3.2-24B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Moin und hallo!"
            }
        ],
        "stream": true,
        "temperature": 0.15,
        "top_k": 10,
        "top_p": 0.5
    }'
```

### `/v1/embeddings`

This route can be used to generate embeddings for text.

```sh
export APIKEY=sk-…

curl -i -X POST https://llm.aihosting.mittwald.de/v1/embeddings
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Qwen3-Embedding-8B",
        "input": "Ein wichtiges Dokument"
    }'
```

Depending on the embedding model used, additional supported parameters such as `dimensions` can be submitted. However, this is not applicable for “Qwen3-Embedding-8B”.

## Error Messages
If the API returns a status code outside the interval from 200 to 399, there is an error in the request. The following errors are to be expected:

- 401 Unauthorized: No API key was transmitted in the Authorization header or the API key is invalid.
- 429 Too Many Requests: The rate limit has been reached. Further requests are not possible within the minute. The rate limit is reset after the minute has elapsed.
- 400 Bad Request: The request cannot be processed. An error message explaining the reasons for this should be provided:
    - At most n image(s) may be provided in one request: The maximum number of images allowed in the entire transmitted context exceeds the capacity of the model. Either images must be removed from the submitted context or a new context must be opened.
    - max_tokens must be at least...: The maximum context length of the model has been exceeded for the generation of text. The error message also displays the number of times the limit has been exceeded as a difference (negative value, number of tokens). 

## Limitations

All requests are subject to a rate limit to ensure fair and consistent usage and availability of the models for all users. During the beta phase, the limit is 300 requests per minute per API key, regardless of the model used. If the limit is reached, the user will receive an HTTP status code `429 Too Many Requests`. After one minute, the limit is reset for the API key used.

All models are also subject to model-specific limitations. These always include the allowed context length, measured in the number of transmitted tokens. For vision-capable models that process images, only a limited number of images can be transmitted per request and thus in the _entire submitted context_. This affects all images transmitted across the entire chat history of a request.

Only the endpoints listed above are implemented with OpenAI API compatibility. Currently, there is no capability to generate images. A corresponding API route does not exist. The following endpoints are not supported:

- `/v1/assistants`
- `/v1/audio`
- `/v1/batches`
- `/v1/evals`
- `/v1/files`
- `/v1/fine_tuning`
- `/v1/images`
- `/v1/moderations`
- `/v1/realtime`
- `/v1/responses`
- `/v1/threads`
- `/v1/uploads`

