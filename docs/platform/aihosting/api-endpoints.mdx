---
sidebar_label: API Usage
description: API Endpoints and Usage Examples
---

## API Endpoints

The API is accessible via HTTPS and is compatible with the OpenAI API. The base URL is `https://llm.aihosting.mittwald.de`. Many applications require specifying the version as well. In this case, the full base URL with versioning should be used: `https://llm.aihosting.mittwald.de/v1`.

Every interaction with the API requires an `Authorization` header with a valid API key. This key can be created in mStudio.

In our examples, we use `curl`, as it is the simplest and quickest way to test. For production use, we recommend using frameworks and libraries that support OpenAI.

### `/v1/models`

This endpoint returns a list of available models.

```sh
export APIKEY=sk-…

curl -i -X GET https://llm.aihosting.mittwald.de/v1/models \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY"
```

It returns a dictionary containing a list of available models. The `id` within this list can be used in the `model` field for subsequent API routes.

### `/v1/chat/completions` and `/v1/completions`

This route allows content to be sent to the LLM in chat format. The endpoint supports streaming for the content generated by the LLM.

```
export APIKEY=sk-…

curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Mistral-Small-3.2-24B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Moin und hallo!"
            }
        ]
    }'

```
The `model` parameter requires a valid model name, which can be retrieved via the `/v1/models` route. Additional model parameters such as `temperature`, `top_p`, or `top_k` can be provided. Recommended settings for these can be found in the model descriptions if they differ from the defaults. Which extended parameters influence the response depends on the model.

To receive a streamed response, the option `stream: true` must be set.

```
curl -i -N -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Mistral-Small-3.2-24B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Moin und hallo!"
            }
        ],
        "stream": true,
        "temperature": 0.15,
        "top_k": 10,
        "top_p": 0.5
    }'
```

### `/v1/embeddings`

This route allows you to generate embeddings for texts.

```
  export APIKEY=sk-…

curl -i -X POST https://llm.aihosting.mittwald.de/v1/embeddings \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Qwen3-Embedding-8B",
        "input": "Ein wichtiges Dokument"
    }'
```

Depending on the embedding model used, additional supported parameters such as `dimensions` can be submitted. However, this is not applicable to “Qwen3-Embedding-8B”.

## Deviations from the OpenAI API

Although our API is compatible with the OpenAI API, there are some limitations:

-   Only selected endpoints are implemented (see list)
    
-   Some parameters like `n`, `logprobs`, `functions` (depending on the model) are not available
    
-   Vision input is currently only supported via Base64, not via URLs
    

### Limitations

All requests are subject to a rate limit to ensure fair and consistent usage and availability of the models for all users. During the beta phase, this is set at 300 requests per minute per API key, regardless of the model used. If the limit is exceeded, the user will receive an HTTP status code `429 Too Many Requests`. After one minute, the limit resets for the used API key.

All models are also subject to model-specific limitations. These always include the allowed context length measured in the number of transmitted tokens. For vision-capable models that process images, only a limited number of images can be transmitted per request, meaning _across the entire submitted context_. This affects all images sent in the full chat history of a request.

Currently, it is **not possible to generate images**. A corresponding API route does not exist.



